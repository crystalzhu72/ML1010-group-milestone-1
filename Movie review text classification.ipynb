{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collect-simpson",
   "metadata": {},
   "source": [
    "# Import raw data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nuclear-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agricultural-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "earned-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path=r\"C:\\Users\\yunan\\Downloads\\York U\\Machine Learning Cert\\ML1010\\aclImdb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accessible-studio",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read train pos files\n",
    "text=[]\n",
    "file_names=[]\n",
    "is_positive=[]\n",
    "#path=r\"C:\\Users\\yunan\\Downloads\\York U\\Machine Learning Cert\\ML1010\\aclImdb\\train\\neg\"\n",
    "#filenames=glob.glob(path + \"/*.txt\")\n",
    "with os.scandir(r\"C:\\Users\\yunan\\Downloads\\York U\\Machine Learning Cert\\ML1010\\Group project 1\\aclImdb\\train\\pos\") as filenames:\n",
    "    for file in filenames:\n",
    "        head, tail = os.path.split(file)\n",
    "        file_names.append(tail)\n",
    "        is_positive.append(1)\n",
    "        with open(file, 'rb') as open_file:\n",
    "            text.append(open_file.readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "competent-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame({'file':file_names, 'text':text, 'is_positive':is_positive})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "floral-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read train neg files\n",
    "with os.scandir(r\"C:\\Users\\yunan\\Downloads\\York U\\Machine Learning Cert\\ML1010\\Group project 1\\aclImdb\\train\\neg\") as filenames:\n",
    "    for file in filenames:\n",
    "        head,tail=os.path.split(file)\n",
    "        file_names.append(tail)\n",
    "        is_positive.append(0)\n",
    "        with open(file,'rb') as open_file:\n",
    "            text.append(open_file.readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ruled-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame({'file':file_names, 'text':text, 'is_positive':is_positive})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "corresponding-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text']=train_df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "racial-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset size - select only 5000 records\n",
    "positive_reviews = train_df[:2500]   #take first 2500 reviews which are positive\n",
    "negative_reviews = train_df[22500:]  #last last 2500 reviews which are negative\n",
    "new_train = positive_reviews.append(negative_reviews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adjacent-intention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>9998_4.txt</td>\n",
       "      <td>b\"Towards the end of the movie, I felt it was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>9999_3.txt</td>\n",
       "      <td>b'This is the kind of movie that my enemies co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>999_3.txt</td>\n",
       "      <td>b\"I saw 'Descent' last night at the Stockholm ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>99_1.txt</td>\n",
       "      <td>b\"Some films that you pick up for a pound turn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>9_1.txt</td>\n",
       "      <td>b\"This is one of the dumbest films, I've ever ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file                                               text  \\\n",
       "0          0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1      10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2     10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3      10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4      10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "...            ...                                                ...   \n",
       "4995    9998_4.txt  b\"Towards the end of the movie, I felt it was ...   \n",
       "4996    9999_3.txt  b'This is the kind of movie that my enemies co...   \n",
       "4997     999_3.txt  b\"I saw 'Descent' last night at the Stockholm ...   \n",
       "4998      99_1.txt  b\"Some films that you pick up for a pound turn...   \n",
       "4999       9_1.txt  b\"This is one of the dumbest films, I've ever ...   \n",
       "\n",
       "      is_positive  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "4995            0  \n",
       "4996            0  \n",
       "4997            0  \n",
       "4998            0  \n",
       "4999            0  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-exception",
   "metadata": {},
   "source": [
    "#  Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "strong-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yunan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt=nltk.WordPunctTokenizer()\n",
    "stop_words=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eleven-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chicken-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer #  Snowball stemmer > Porter stemmer\n",
    "from nltk.tokenize.casual import casual_tokenize  # we use casual tokenize because this is colloquial text\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words  # sklearn stop words is larger than nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "continuing-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def decontracted(phrase):\n",
    "    # Taken from https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def tokenize_phrase(text):\n",
    "    phrase = decontracted(text.replace('\\\\', '').replace(\"b'\", \"\")) # remove backslashes and replace contractions\n",
    "    tokens = casual_tokenize(phrase, reduce_len=True, strip_handles=True)\n",
    "    normalized_tokens = [x.lower() for x in tokens] #  convert to all lowercase\n",
    "    filtered_tokens = [x for x in normalized_tokens if x not in sklearn_stop_words] #  filter stop words\n",
    "    filtered_tokens = [x for x in filtered_tokens if x and x not in '- \\t\\n.\"\\':[...][\\\\]()/[br]<>*~,;!?'] #  filter punctuations\n",
    "    stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens] # perform stemming\n",
    "    return stemmed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "conventional-shaft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "      <td>[homeless, houseless, georg, carlin, state, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[brilliant, over-act, lesley, ann, warren, bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[easili, underr, film, inn, brook, cannon, sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[typic, mel, brook, film, slapstick, movi, act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>9998_4.txt</td>\n",
       "      <td>b\"Towards the end of the movie, I felt it was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[end, movi, felt, technic, felt, like, classro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>9999_3.txt</td>\n",
       "      <td>b'This is the kind of movie that my enemies co...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kind, movi, enemi, content, watch, time, bloo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>999_3.txt</td>\n",
       "      <td>b\"I saw 'Descent' last night at the Stockholm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[saw, descent, night, stockholm, film, festiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>99_1.txt</td>\n",
       "      <td>b\"Some films that you pick up for a pound turn...</td>\n",
       "      <td>0</td>\n",
       "      <td>[film, pick, pound, turn, good, 23rd, centuri,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>9_1.txt</td>\n",
       "      <td>b\"This is one of the dumbest films, I've ever ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dumbest, film, seen, rip, near, type, thrille...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file                                               text  \\\n",
       "0          0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1      10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2     10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3      10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4      10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "...            ...                                                ...   \n",
       "4995    9998_4.txt  b\"Towards the end of the movie, I felt it was ...   \n",
       "4996    9999_3.txt  b'This is the kind of movie that my enemies co...   \n",
       "4997     999_3.txt  b\"I saw 'Descent' last night at the Stockholm ...   \n",
       "4998      99_1.txt  b\"Some films that you pick up for a pound turn...   \n",
       "4999       9_1.txt  b\"This is one of the dumbest films, I've ever ...   \n",
       "\n",
       "      is_positive                                             tokens  \n",
       "0               1  [bromwel, high, cartoon, comedi, ran, time, pr...  \n",
       "1               1  [homeless, houseless, georg, carlin, state, is...  \n",
       "2               1  [brilliant, over-act, lesley, ann, warren, bes...  \n",
       "3               1  [easili, underr, film, inn, brook, cannon, sur...  \n",
       "4               1  [typic, mel, brook, film, slapstick, movi, act...  \n",
       "...           ...                                                ...  \n",
       "4995            0  [end, movi, felt, technic, felt, like, classro...  \n",
       "4996            0  [kind, movi, enemi, content, watch, time, bloo...  \n",
       "4997            0  [saw, descent, night, stockholm, film, festiv,...  \n",
       "4998            0  [film, pick, pound, turn, good, 23rd, centuri,...  \n",
       "4999            0  [dumbest, film, seen, rip, near, type, thrille...  \n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train['tokens'] = new_train['text'].apply(tokenize_phrase)\n",
    "new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-performer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-ecuador",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-breakfast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "arabic-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8021155d7e25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcasual_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrip_handles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\fortensorflow\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36mcasual_tokenize\u001b[1;34m(text, preserve_case, reduce_len, strip_handles)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[0mConvenience\u001b[0m \u001b[0mfunction\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwrapping\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m     return TweetTokenizer(\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mpreserve_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreserve_case\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrip_handles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrip_handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     ).tokenize(text)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fortensorflow\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m    285\u001b[0m         \u001b[1;31m# Fix HTML character entities:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_replace_html_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[1;31m# Remove username handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip_handles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\fortensorflow\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36m_replace_html_entities\u001b[1;34m(text, keep, remove_illegal, encoding)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mremove_illegal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mENT_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_entity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_str_to_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "tokens = casual_tokenize(new_train, reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-intranet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-somerset",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens=tokenize(new_train.readline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(split):\n",
    "    if split.lower() == 'train':\n",
    "        folder = 'train'\n",
    "    elif split.lower() == 'test':\n",
    "        folder = 'test'\n",
    "    else:\n",
    "        raise ValueError('Invalid data split specified.')\n",
    "        \n",
    "    file_names = []\n",
    "    text = []\n",
    "    is_positive = []\n",
    "    \n",
    "    # read all positive files\n",
    "    files = glob.glob(os.path.join(\"data\", folder, 'pos', '*'))\n",
    "    for file in files:\n",
    "        head, tail = os.path.split(file)\n",
    "        file_names.append(tail)\n",
    "        is_positive.append(1)\n",
    "        with open(file, 'rb') as open_file:\n",
    "            text.append(open_file.readlines()[0])\n",
    "            \n",
    "    # read all negative files\n",
    "    files = glob.glob(os.path.join(\"data\", folder, 'neg', '*'))\n",
    "    for file in files:\n",
    "        head, tail = os.path.split(file)\n",
    "        file_names.append(tail)\n",
    "        is_positive.append(0)\n",
    "        with open(file, 'rb') as open_file:\n",
    "            text.append(open_file.readlines()[0])\n",
    "            \n",
    "    return pd.DataFrame(data={'file': file_names, 'text': text, 'is_positive': is_positive})\n",
    "\n",
    "\n",
    "train_df = get_data('train')\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-interval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-ghost",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-confusion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
